%! TEX root = ../000-main.tex
\section{Study of Convergence}%
\label{sec:convergence}

\subsection{Experiment Setup}%
\label{sub:convergence_setup}

We'll study the global and local convergence of
the three algorithms only in terms of the objective function
$\tilde L$. In order to do so, we'll use a small dataset
to reduce the computational cost of the experiments. The
parameters to generate the dataset are the following:
\begin{center}
    \begin{BVerbatim}
    tr_p = 250; te_q = 250; tr_freq = 0.5;
    \end{BVerbatim}
\end{center}

We'll run the program for all combinations of target digit,
$\lambda$, and algorithm shown below:
\begin{alignat*}{3}
    \text{digit} &\in \{0, 1, 2, 3, 4, 5, 6, 7, 8, 9\} & \quad
    \lambda &\in \{0, 0.01, 0.1\} & \quad
    \text{algorithm} &\in \{\text{GM}, \text{QNM}, \text{SGM}\}
\end{alignat*}
Which amounts to $10 \cdot 3 \cdot 3 = 90$ instances to be solved. To
ensure that the results are reproducible, we'll use the following seeds:
\begin{center}
    \begin{BVerbatim}
        tr_seed=48089260; te_seed=26060125; sg_seed=565544;
    \end{BVerbatim}
\end{center}
Finally, the maximum number of iterations will be set to \texttt{kmax=1000}
(\texttt{sg\_emax} for \texttt{SGM}).

Some other parameters are fixed to the following values:
\begin{center}
    \begin{BVerbatim}
epsG = 10^-6;                                                
ils=3; ialmax = 1; kmaxBLS=10; epsal=10^-3; c1=0.01; c2=0.45;
sg_al0 = 2; sg_be = 0.3; sg_ga = 0.01;
sg_ebest = floor(0.01*sg_emax);
\end{BVerbatim}
\end{center}

The complete implementation can be found in the source code
of the project. To run all the experiments we used
\texttt{uo\_nn\_batch}. All the results are in \cref{sec:appendix}

\pagebreak
\subsection{Global convergence}

\subsubsection{Objective function}%
\label{ssub:obj_func}

\Cref{fig:lambda_L} shows the relationship between the parameter
$\lambda$ and the value of the objective function $\tilde L$ at
the convergence of the algorithms ($L^* = L(w^*)$). There is a clear positive
trend between $\lambda$ and $L^*$ for all three algorithms.

\begin{figure}[ht]
\input{../analysis/lambda_L.tikz}
\caption{Value of $L^*$ as a function of $\lambda$ for the three algorithms.}%
\label{fig:lambda_L}
\end{figure}

We can also see that $QNM$ and $GM$ behave similarly, in fact, they
converge to the same value of $L^*$ in most cases. On the other
hand, $SGM$ is consistently worse in terms of the value of $L^*$ than
the other two algorithms.

The best combination of algorithm and $\lambda$ is $GM$ (Gradient method)
with $\lambda = 0$. Obtaining the best value of $L^*=10^{-5}$, as shown
in \cref{tab:lambda_L}.

% Global convergence: analyse the global convergence of every algorithm and how
% this global convergence property depends on the value of the regularization parameter
% í µí¼†í µí¼†. Which combination algorithm-í µí¼†í µí¼† gives the best results in terms of global converge? In
% particular, discuss the application to the SGM of the conditions for global convergence.

\begin{table}[ht]
    \caption{Average value of $L^*$ with standard deviation $\sigma$ for each algorithm and $\lambda$}%
    \label{tab:lambda_L}%
    \input{global_conv_table}
\end{table}

\subsubsection{Accuracy}\label{ssub:acc_conv}

\Cref{fig:lambda_acc} shows the relationship between the regularization
parameter $\lambda$ and the accuracy on the train and test sets. As expected,
when $\lambda$ increases, the accuracy on the train and test set decreases,
this is consistent with the results from the previous section, where we found
that the value of the objective function $L^*$ increases with $\lambda$.

However, the accuracy on the \emph{train} set for $SGM$ is near 50\% for $SGM$ with
$\lambda = 0.1$, while on the \emph{test} set is much higher around 93\%. This
seems counter-intuitive, but can be explained by the fact that the train
set is unbalanced having 50\% of the samples being the target digit while
the test set is 10\%. Nonetheless, these values indicate that $SGM$ is probably
classifying all as false, hence the values being close to the proportion of
the target on the data.

\begin{figure}[H]
\input{../analysis/lambda_acc.tikz}
\caption{Accuracy on train and test error for each algorithm and $\lambda$}%
\label{fig:lambda_acc}
\end{figure}

% \begin{figure}[ht]
% \input{../analysis/iter_alg.tikz}
% \caption{Number of iterations for each algorithm}%
% \label{fig:iter_alg}
% \end{figure}

\pagebreak
\subsection{Local convergence}

\subsubsection{Speed of convergence}

To study the speed of convergence, we'll start by looking at
the boxplots of execution time of the algorithms as well as
the number of iterations. The results are shown in \cref{fig:tex_niter}.

Note that the iteration and execution time values are in logarithmic scale
to better visualize the results.

By execution time, $GM$ is the slowest, followed by $QNM$ and then $SGM$ which
if the fastest. However, there is a huge variability in the execution times
of the algorithms and there are some notable outliers (specially in the case
of $SGM$).

The number of iterations shows that $SGM$ takes the most iterations to
reach the results. This is expected since the iterations of $SGM$ are
counting the inner iterations of the mini-batches and as such cannot
be directly compared to the other two algorithms. One thing to notice
is that there are some outliers for $SGM$ several orders of magnitude
above the expected number of iterations, indicating that in some cases
the algorithm is not converging. $QNM$ converges in fewer iterations
than $GM$.

\begin{figure}[H]
\input{../analysis/tex_niter.tikz}
\caption{Execution time (left) and iterations (right) for each algorithm}%
\label{fig:tex_niter}
\end{figure}

\subsubsection{Effect of the regularization parameter on the speed of convergence}

We will now take produce the same plot as in~\cref{fig:tex_niter} but
also showing the $\lambda$ parameter. The results are shown
in~\cref{fig:lambda_tex_niter}.

% If we now look at the execution time for each algorithm and
% how it depends on $\lambda$ as shown in \cref{fig:lambda_tex_niter},
% where we show the execution time on the left subplot and the
% number of iterations on the right subplot.

We can see that
when $\lambda = 0$, there is a wide range of execution times
across all algorithms, but the fastest algorithm is $QNM$.

When using a regularizer, the execution time is more consistent
and the fastest algorithm is $SGM$ by a noticeable margin.
This is despite the fact that $SGM$ has more iterations than the
other two methods, but as we will see, the execution time per
iteration is much lower for $SGM$, since the computations are
much simpler.

\begin{figure}[H]
\input{../analysis/lambda_tex_niter.tikz}
\caption{Execution time (left) and number of iterations (right) for each algorithm and $\lambda$ combination}%
\label{fig:lambda_tex_niter}
\end{figure}


% Compare the speed of convergence of the three algorithms in terms of the
% execution time and number of iterations.

\Cref{fig:lambda_iter} shows the number of iterations for each
combination of algorithm and $\lambda$ for the different target
numbers.
The iterations are in logarithmic scale and we show
the maximum number of iterations for each algorithm as a red line
(1000 for $GM$ and $QNM$ and 125000 for $SGM$).

This is the same data shown in the \texttt{niter}
subplot of \cref{fig:lambda_tex_niter}, but showing all the datapoints
for each digit instead of the boxplots. It allows us to visualize
what are the outliers and stable are the algorithms.

\begin{figure}[H]
\input{../analysis/lambda_iter.tikz}
\caption{Number of iterations for each algorithm}%
\label{fig:lambda_iter}
\end{figure}

The first thing we observe is that a bigger $\lambda$ reduces
the number of iterations. This is expected since the regularization
parameter $\lambda$ is a trade-off between the data fitting and
the smoothness of the solution. The exception are some cases in
$QNM$ where we seem to converge in less than 10 iterations.

The iterations for $QNM$ and $GM$
are in the same order of magnitude but we can see some
notable differences. For instance, $GM$ takes more iterations
in most cases. Also, for $\lambda = 0$ and target number 8,
$GM$ takes reaches the maximum number of iterations (1000), while
$QNM$ converges in less than 100 iterations in all but two cases.

Another interesting observation is that the differences between
iterations between digits are more consistent in $QNM$, with much
less variability. The exception is when $\lambda = 0$, where
$QNM$ takes either less iterations than with the regularizer or
more iterations.

For $SGM$, we cannot compare the number of iterations with the
other two algorithms, but we can see that with $\lambda = 0$,
$SGM$ does not converge in 5 instances, while it converges in
the rest.

% Analyse how the speed of convergence of the three algorithms depend on the
% value of lambda and try to find an explanation for the observed dependence,
% if any.

% Analyze the running time per iteration and try to find an explanation for
% the different values among the three algorithms.

\subsubsection{Execution time per iteration}%
\label{ssub:time_per_iter}

In the previous section, we have seen the number of iterations
and the execution time for each algorithm separately (\cref{fig:tex_niter}).
Let us now look at relationship between the two: the execution time per iteration for each
algorithm and how it depends on $\lambda$. Since the execution time
of $SGM$ is much lower than the other two, we cannot plot it in the
same scale. In \cref{fig:lambda_tex_over_niter_comb} we show the
boxplots of the execution time per iteration for each algorithm
and $\lambda$, notice that for $GM$ and $QNM$ we use
\si{\milli\second} ($10^{-3}$s) as the unit of time, while for $SGM$ we use
\si{\micro\second} ($10^{-6}$s).

The fastest execution time per iteration is for $SGM$, with a mean
of \SI{13}{\micro\second} per iteration. It seems that the effect
of the regularizer is negligible for $SGM$, with no noticeable change
between the different values of $\lambda$.

For $GM$ and $QNM$, we see a clear effect of the regularizer on
the execution time per iteration. What's interesting is that
not only is there a noticeable difference between not having
a regularizer and having one, but also between
different values of $\lambda$. This may be caused by the fact that
not all the time we measure is part of the actual iterations of the
algorithm, but it also includes the initialization of the algorithm,
memory allocation, etc. This means that with higher values of $\lambda$,
when the algorithm converges faster the execution time per iteration
is higher since the initialization time is a bigger part of the total.

\begin{figure}[htb]
    \input{../analysis/lambda_tex_over_niter_comb.tikz}
    \caption{Execution time per iteration as a function of $\lambda$ for the three algorithms.}%
    \label{fig:lambda_tex_over_niter_comb}
\end{figure}

\subsubsection{Using epochs for SGM}%
\label{ssub:epochs}

In order to compare the iterations of $SGM$ with the other two,
we can count the number of epochs of $SGM$ instead of the total iterations.
This can be done given that the number of iterations is given by the
following formula:
\begin{equation}\label{eq:ksg}
    k^{SG} = e^{SG} \cdot k^{SG}_e
\end{equation}
where $k^{SG}$ is the total number of iterations, $e^{SG}$ is the number of
epochs and $k^{SG}_e$ is the number of iterations per epoch. Since the number
of iterations per epoch is constant and depends on the number of samples
($p = 250$) and $\gamma^{SG} = 0.01$, we can compute the number of epochs
as follows:
\begin{equation}\label{eq:esg}
    \begin{rcases}
        k^{SG}_e &= \left\lceil\frac{p}{\lfloor \gamma^{SG} \cdot p \rfloor}\right\rceil \\
        \gamma^{SG} &= 0.01 \\
        p &= 250
    \end{rcases} k^{SG}_e = 125 \xRightarrow{\text{\cref{eq:ksg}}} \boxed{e^{SG} = \frac{k^{SG}}{125}}
\end{equation}
From~\cref{eq:esg} we determined that 125 iterations correspond to one epoch. We can
now plot the execution time per epoch for $SGM$ and compare it with $GM$ and $QNM$.
The revised plot is shown in \cref{fig:lambda_tex_over_niter_comb2}. When comparing
the $SGM$ epochs to the iterations of the other algorithms, we can see that $SGM$ epochs
are slower than iterations of $GM$ but faster than iterations of $QNM$ for
$\lambda \in \{0, 0.01\}$, but for $\lambda = 0.1$ $SGM$ is faster than $GM$ too. This
is due to the fact that the value of $\lambda$ does not affect the execution time of
the epochs of $SGM$, as we discussed previously.


\begin{figure}[H]
    \input{../analysis/lambda_tex_over_nie.tikz}
    \caption{Execution time per iteration as a function of $\lambda$ for the three algorithms. Using
    epochs for $SGM$}%
    \label{fig:lambda_tex_over_niter_comb2}
\end{figure}

\subsection{Discussion}%
\label{ssub:discussion}

Our objective was to answer the question of which combination of algorithm and $\lambda$
is the most efficient, we have seen that the global convergence of
$GM$ and $QNM$ is better than $SGM$ for all values of $\lambda$
(\cref{fig:lambda_L,tab:lambda_L}). However,
the execution time of $SGM$ is much lower than the other two
(\cref{fig:lambda_tex_niter,fig:lambda_tex_over_niter_comb}).

Given all that, we have to consider whether the additional computation
time of $GM$ and $QNM$ is worth the improvement in global convergence.
The answer to this question depends on the application, but we argue that
by what we have seen in our problem of $SLNN$: $SGM$ with $\lambda=0.01$ is the best choice;
it is much faster and gives similar results to the other two algorithms.
We could also use $\lambda=0.1$, but probably not $\lambda=0$, since
we have seen that it reached the maximum number of iterations in some cases.
